Multimodal-understadning:
LLMs: an LLM is an autoregressive probabilistic model that predict the most possible next token conditioned on user input and prior generated tokens, <eq1: p(w)=\prod_{i=1}^n p_{\theta} (w_i|w_{<i})>, where theta is the params of the LLM, which generally consists of multiple layers of transformers.
Image LLMs: Image LLMs equip the LLM the ablity to read and understand images, and the archtectures can be catogorized into two architecture: alignment and early fusion~\cite{chen2024multi}. Alignment architecture Image LLMs treats the input image as an additional extension. The vision encoder extract information from the image, while the trainable alignment module aims to fuse the extracted features and the lexcial tokens and fianlly pass throught the underlying LLM; The early-fussion architecture aims to train a multimodal LLM from scratch where both text and images are converted into tokens.

Multimodal-generation:
Diffusion probablistic modeling: Talks about DDPM mainly here, the DDPM includes a forward and a backward process, the forward process iteratively add random gausian noise to the input image x_0 (markov process), for given timestep t, <eq2:q(x_t|x_t-1)=\mathcal{N}(x_t|\sqrt{1-\beta_t}x_t-1,\beta_t I)>, where \beta_t \in (0,1) is chosen based on a schedule and generally \beta_1 < \beta_2 < .. < \beta_T. T is usually large that the so that the x_T is closed to a guassian noise. To generate image from random noise, We need to reverse the diffusion process by denoise the random noise iteratively by using a neural network as in <eq3:p_\theta(x_t-1|x_t)=\mathcal{N}(x_t-1|\mu_\theta(x_t,t),\beta_t I)> where \mu_theta is the neural network parameterized by \theta which takes x_t and timestep t as input that trys to predict the denoised x_t-1. During training, the objective function is generally adopted as <eq4:min_theta \mathbb{E}_x_0,\straightepsilon,t[w_t||\straightepsilon-\straightepsilon_\theta(x_t,t)||_2^2]>, where \straightepsilon is the randomly sampled noise, \straightepsilon_theta is the neural netwoks to predict the noise, instead of x_t-1.
Latent Diffusion: However, DMs are prone to spend excessove amount of capacity and computing resources on imperceptible details of the data. LDM~\cite{rombach2022high} aims to reduce the significatnt resources consumption without impact the performance by training DMs on lower dimension latent space $z=E(x)$ compressed by an pretrained autoencoder on the pixel space. The objective function is then <eq5:min_theta \mathbb{E}_{z_o=E_(x0),\straightepsilon,t}[w_t||\straightepsilon-\straightepsilon_\theta(z_t,t,c)||_2^2], where the c in the denoising network is an additional input for conditional generation, such as text prompts in text-to-image tasks.


Multimodal understanding AND generation?:(Very important part in this report)
Joint AR and diffusion model: Consider the ablity of LLMs for text gen and DMs for image generation, one natural way to think about multimodal understanding and generation is to combine a LLM and a DM. Typically, there are two possible architectures~\cite{chen2024multi}. The first one is that we have a pretrained Image LLM and DM, we can either simply connect the two by using DMs as a tool for LLMs to control, which often suffers from multi-modal generation conditions, or we can train a connector that aligns the DM and the LLM. However, this architechture is still limited by independent modeling. Another possible way is to use a unified multimodal transformer architechture to train with both diffusion and autoregressive regularization. In Transfusion~\cite{zhou2024transfusion}, text are processed using embedding matrices, and local $k\times k$ image patch are compressed into a single transformer vector using neural networks, with a hybrid attention machanism (causal + bidirectional). In denoising mode, a pure noise in the form of n image patches is appended  to the input sequence, and denoised over T steps using the transformer model.
Autoregressive image generation: In autoregressive image generation, previous attemps like Chemeleon~\cite{team2024chameleon}, aims to tokenize images into discrete tokens from a codebook and mix them with text tokens and fed to a unifed  multimodal autoregressive model. And the model will then output some mixed text and image tokens. Despite the efforts, this method still faces two important problems~\cite{chen2024multi}. First is that the current codebook of the tokenizers are obtained through the image reconstruction objective, which means that it contains more pixel level information instead of semantic information; Secoundly, the discretization will inevitably lose visual information, which might cause failure in tasks like fine-grained image understanding.
Autoregressive without vector quantization: MAR~\cite{li2024autoregressive} proposed a way to generate images using masked autogressive model without the quantization of image tokens, which models the per-token probability distribution by a diffusion procedure on continous-valued domain. In traditional autoregressive generation models, the generated continous-valued vector z is projected by a K-way classifier matrix onto discrete domain, essentially a categorical probability distribution in the form of $p(x|z)=softmax(Wz)$; however, we can also formulate the probailty of p(x|z) as a denoising criteron <eq6:L(z,x)=\mathecal{E}_\straightepsilon,t[||\straightepsilon-\straightepsilon_\theta(x_t,t,z)||_2^2]>.

Challenges in comic gen?
Despite the advancement in controllable generation with diffusion models, like Textual Inversion~\cite{gal2022image} and Dreambooth~\cite{ruiz2023dreambooth}, most commercially available image generation systems aimed at end users rerely expose training-time customization, which hinders the ability to consistently represent the same art style of characters acrooss scenes. Moreover, traditional diffusion based image generation models have limited capabilities for accurate text rendering within images. Notably, OpenAI's new ChatGPT-4o (March 25, 2025)~\cite{openai2025introducing} with the image generation capability is announced to be a unified multimodal autoregressive model, which is capable of generate clear text and consistent characters, without additional traning, which is essentila for comic creation.


